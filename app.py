import os
import time
import uuid
import json
import random
from flask import Flask, request, jsonify, render_template, url_for
import requests
from PIL import Image
import numpy as np
import torch
from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler, StableVideoDiffusionPipeline
import cv2
import gc

class VideoGenerator:
    """ç¨ç«‹çš„å½±ç‰‡ç”Ÿæˆé¡åˆ¥"""
    
    def __init__(self, device=None, torch_dtype=None):
        self.video_pipe = None
        self.device = device or "cpu"
        self.torch_dtype = torch_dtype or torch.float32
        self.video_loaded = False
        
    def load_video_model(self):
        """è¼‰å…¥å½±ç‰‡ç”Ÿæˆæ¨¡å‹"""
        if self.video_loaded:
            return True
            
        try:
            print("ğŸ¬ è¼‰å…¥ Stable Video Diffusion æ¨¡å‹...")
            video_model_id = "stabilityai/stable-video-diffusion-img2vid-xt"
            
            video_kwargs = {
                "torch_dtype": self.torch_dtype
            }
            
            # é‡å°ä¸åŒè¨­å‚™çš„å„ªåŒ–
            if self.device != "mps" and self.torch_dtype == torch.float16:
                video_kwargs["variant"] = "fp16"
            
            self.video_pipe = StableVideoDiffusionPipeline.from_pretrained(
                video_model_id, **video_kwargs
            ).to(self.device)
            
            # è¨˜æ†¶é«”å„ªåŒ–è¨­å®š
            if self.device == "cuda":
                self.video_pipe.enable_model_cpu_offload()
                self.video_pipe.enable_vae_slicing()
            elif self.device == "mps":
                self.video_pipe.enable_attention_slicing(1)
            else:
                self.video_pipe.enable_attention_slicing()
            
            self.video_loaded = True
            print(f"âœ… å½±ç‰‡ç”Ÿæˆæ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: {self.device}")
            return True
            
        except Exception as e:
            print(f"âŒ å½±ç‰‡æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            self.video_pipe = None
            self.video_loaded = False
            return False
    
    def generate_video_from_image(self, image_path, static_dir, story_text=""):
        """å¾åœ–åƒç”Ÿæˆå½±ç‰‡"""
        try:
            # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
            if not self.load_video_model() or self.video_pipe is None:
                print("âŒ å½±ç‰‡ç”Ÿæˆæ¨¡å‹æœªè¼‰å…¥")
                return None
            
            # è¼‰å…¥åœ–åƒ
            full_image_path = os.path.join(static_dir, image_path)
            if not os.path.exists(full_image_path):
                print(f"âŒ æ‰¾ä¸åˆ°åœ–åƒæ–‡ä»¶: {full_image_path}")
                return None
            
            input_image = Image.open(full_image_path)
            
            # ç¢ºä¿åœ–åƒç‚ºRGBæ¨¡å¼
            if input_image.mode != 'RGB':
                input_image = input_image.convert('RGB')
            
            # èª¿æ•´åœ–åƒå°ºå¯¸ï¼ˆSVD éœ€è¦ç‰¹å®šå°ºå¯¸æ¯”ä¾‹ï¼‰
            target_width, target_height = 1024, 576
            input_image = input_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
            
            print("ğŸ¬ é–‹å§‹ç”Ÿæˆå½±ç‰‡...")
            
            # ç”Ÿæˆåƒæ•¸ï¼ˆé‡å°é€Ÿåº¦å„ªåŒ–ï¼‰
            video_params = {
                "image": input_image,
                "decode_chunk_size": 2,  # è¼ƒå°çš„chunk sizeæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
                "generator": torch.Generator(device=self.device).manual_seed(42),
                "motion_bucket_id": 127,  # ä¸­ç­‰é‹å‹•å¼·åº¦
                "noise_aug_strength": 0.02,  # è¼ƒä½çš„å™ªè²å¢å¼·ä»¥æé«˜ç©©å®šæ€§
                "num_frames": 25,  # æ¨™æº–å¹€æ•¸
            }
            
            # é‡å°ä¸åŒè¨­å‚™èª¿æ•´åƒæ•¸
            if self.device == "cpu":
                video_params["num_frames"] = 14  # CPUæ¨¡å¼æ¸›å°‘å¹€æ•¸
                video_params["decode_chunk_size"] = 1
            elif self.device == "mps":
                video_params["decode_chunk_size"] = 4
            
            start_time = time.time()
            
            # æ¸…ç†è¨˜æ†¶é«”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            with torch.no_grad():
                frames = self.video_pipe(**video_params).frames[0]
            
            generation_time = time.time() - start_time
            print(f"ğŸ¬ å½±ç‰‡å¹€ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {generation_time:.2f}ç§’ï¼Œå¹€æ•¸: {len(frames)}")
            
            if not frames or len(frames) == 0:
                print("âŒ å½±ç‰‡å¹€ç”Ÿæˆå¤±æ•—")
                return None
            
            # ä¿å­˜å½±ç‰‡
            timestamp = int(time.time())
            random_id = str(uuid.uuid4())[:8]
            output_filename = f"dream_video_{timestamp}_{random_id}.mp4"
            
            output_dir = os.path.join(static_dir, 'videos')
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, output_filename)
            
            # ä½¿ç”¨ OpenCV ä¿å­˜å½±ç‰‡
            fps = 8  # 8 FPS
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (target_width, target_height))
            
            for frame in frames:
                if frame is None:
                    continue
                frame_array = np.array(frame)
                if frame_array.shape[2] == 3:  # RGB
                    frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)
                else:
                    frame_bgr = frame_array
                out.write(frame_bgr)
            
            out.release()
            
            # é©—è­‰å½±ç‰‡æ–‡ä»¶
            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                print(f"âœ… å½±ç‰‡ä¿å­˜æˆåŠŸ: {output_filename}")
                return os.path.join('videos', output_filename)
            else:
                print("âŒ å½±ç‰‡æ–‡ä»¶ä¿å­˜å¤±æ•—æˆ–æ–‡ä»¶ç‚ºç©º")
                return None
                
        except Exception as e:
            print(f"âŒ å½±ç‰‡ç”Ÿæˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def clear_video_memory(self):
        """æ¸…ç†å½±ç‰‡æ¨¡å‹è¨˜æ†¶é«”"""
        if self.video_pipe is not None:
            del self.video_pipe
            self.video_pipe = None
        
        self.video_loaded = False
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        print("ğŸ¬ å½±ç‰‡æ¨¡å‹è¨˜æ†¶é«”å·²æ¸…ç†")


class DreamAnalyzer:
    def __init__(self):
        self.app_root = os.path.dirname(os.path.abspath(__file__))
        self.static_dir = os.path.join(self.app_root, 'static')
        self.app = Flask(__name__, 
                        template_folder=os.path.join(self.app_root, 'templates'),
                        static_folder=self.static_dir)
        
        # é…ç½®
        self.OLLAMA_API = "http://localhost:11434/api/generate"
        self.OLLAMA_MODEL = "qwen2.5:14b"
        
        # æ¨¡å‹ç‹€æ…‹
        self.image_pipe = None
        self.models_loaded = False
        self.current_device = None
        self.torch_dtype = None
        
        # å½±ç‰‡ç”Ÿæˆå™¨
        self.video_generator = None
        
        # å¯ç”¨æ¨¡å‹
        self.models = {
            "stable-diffusion-v1-5": "runwayml/stable-diffusion-v1-5",
        }
        
        # é˜²é‡è¤‡æäº¤
        self.processing_requests = set()  # æ­£åœ¨è™•ç†çš„è«‹æ±‚ID
        self.request_lock = False  # å…¨å±€é–
        
        self._setup_routes()
        self._create_directories()

    def _create_directories(self):
        """å‰µå»ºå¿…è¦çš„ç›®éŒ„"""
        directories = [
            os.path.join(self.static_dir, 'images'),
            os.path.join(self.static_dir, 'videos'),  # æ–°å¢å½±ç‰‡ç›®éŒ„
            os.path.join(self.static_dir, 'shares')
        ]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)

    def _setup_routes(self):
        """è¨­å®šè·¯ç”±"""
        self.app.route('/')(self.index)
        self.app.route('/api/status')(self.api_status)
        self.app.route('/api/analyze', methods=['POST'])(self.analyze)
        self.app.route('/api/share', methods=['POST'])(self.share_result)
        self.app.route('/share/<share_id>')(self.view_shared)

    def _initialize_device(self):
        """åˆå§‹åŒ–è¨­å‚™è¨­å®š"""
        if torch.cuda.is_available():
            self.current_device = "cuda"
            self.torch_dtype = torch.float16
        elif torch.backends.mps.is_available():
            self.current_device = "mps"
            self.torch_dtype = torch.float32
        else:
            self.current_device = "cpu"
            self.torch_dtype = torch.float32

    def _initialize_video_generator(self):
        """åˆå§‹åŒ–å½±ç‰‡ç”Ÿæˆå™¨"""
        if self.video_generator is None:
            self._initialize_device()
            self.video_generator = VideoGenerator(
                device=self.current_device, 
                torch_dtype=self.torch_dtype
            )
        return self.video_generator

    def _load_image_model(self):
        """è¼‰å…¥åœ–åƒç”Ÿæˆæ¨¡å‹"""
        if self.models_loaded:
            return True
        
        try:
            self._initialize_device()
            model_id = "runwayml/stable-diffusion-v1-5"
            
            self.image_pipe = StableDiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=self.torch_dtype,
                use_safetensors=True,
                safety_checker=None,
                requires_safety_checker=False
            ).to(self.current_device)
            
            self.image_pipe.scheduler = UniPCMultistepScheduler.from_config(
                self.image_pipe.scheduler.config
            )
            
            # å„ªåŒ–è¨­å®š
            if self.current_device == "cuda":
                self.image_pipe.enable_model_cpu_offload()
                self.image_pipe.enable_attention_slicing()
                self.image_pipe.enable_vae_slicing()
            elif self.current_device == "mps":
                self.image_pipe.enable_attention_slicing(1)
            else:
                self.image_pipe.enable_attention_slicing()
            
            self.models_loaded = True
            print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨è¨­å‚™: {self.current_device}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return False

    def _check_ollama_status(self):
        """æª¢æŸ¥ Ollama æœå‹™ç‹€æ…‹"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False

    def _call_ollama(self, system_prompt, user_prompt, temperature=0.7):
        """èª¿ç”¨ Ollama API"""
        try:
            data = {
                "model": self.OLLAMA_MODEL,
                "prompt": user_prompt,
                "system": system_prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "top_p": 0.9,
                    "num_predict": 500
                }
            }
            
            response = requests.post(self.OLLAMA_API, json=data, timeout=120)
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            return ""
        except Exception as e:
            print(f"âŒ Ollama èª¿ç”¨å¤±æ•—: {e}")
            return ""

    def _generate_story(self, dream_text):
        """ç”Ÿæˆå¤¢å¢ƒæ•…äº‹"""
        system_prompt = """ä½ æ˜¯å¤¢å¢ƒæ•…äº‹å‰µä½œå°ˆå®¶ï¼Œè¦æ±‚ï¼š
1. ç›´æ¥é–‹å§‹æ•…äº‹ï¼Œç„¡å•å€™èª
2. èåˆå¤¢å¢ƒå…ƒç´ 
3. ä½¿ç”¨ç¬¬ä¸€äººç¨±
4. 150-200å­—å®Œæ•´æ•…äº‹
5. ç¹é«”ä¸­æ–‡"""

        user_prompt = f"åŸºæ–¼å¤¢å¢ƒç‰‡æ®µå‰µä½œæ•…äº‹ï¼šã€Œ{dream_text}ã€"
        
        story = self._call_ollama(system_prompt, user_prompt)
        return self._clean_story_content(story) if story else "ç„¡æ³•ç”Ÿæˆå¤¢å¢ƒæ•…äº‹"

    def _clean_story_content(self, story):
        """æ¸…ç†æ•…äº‹å…§å®¹"""
        unwanted_phrases = [
            "å¥½çš„ï¼Œæ ¹æ“šæ‚¨çš„å»ºè­°", "æ ¹æ“šæ‚¨çš„è¦æ±‚", "ä»¥ä¸‹æ˜¯æ•…äº‹", "æ•…äº‹å¦‚ä¸‹",
            "###", "**", "æ•…äº‹åç¨±ï¼š", "å¤¢å¢ƒæ•…äº‹ï¼š", "å®Œæ•´æ•…äº‹ï¼š"
        ]
        
        cleaned_story = story.strip()
        
        for phrase in unwanted_phrases:
            if cleaned_story.startswith(phrase):
                cleaned_story = cleaned_story[len(phrase):].strip()
            if phrase in cleaned_story:
                parts = cleaned_story.split(phrase)
                if len(parts) > 1:
                    cleaned_story = parts[-1].strip()
        
        # ç§»é™¤å¼•è™Ÿ
        if cleaned_story.startswith('"') and cleaned_story.endswith('"'):
            cleaned_story = cleaned_story[1:-1].strip()
        if cleaned_story.startswith('ã€Œ') and cleaned_story.endswith('ã€'):
            cleaned_story = cleaned_story[1:-1].strip()
        
        return cleaned_story.replace('*', '').replace('#', '').strip()

    def _generate_image_prompt(self, dream_text):
        """å°‡å¤¢å¢ƒè½‰æ›ç‚ºåœ–åƒç”Ÿæˆæç¤ºè©"""
        system_prompt = """You are a Stable Diffusion prompt expert. Convert user input into English image generation prompts. Requirements:
        1. PRESERVE and translate ALL original elements from the input
        2. If input is a story/emotion, extract the MOST VISUAL and EMOTIONAL scene
        3. ENHANCE with additional quality details, don't replace original content
        4. Use English keywords only
        5. The main characters are mostly Asian
        6. Focus on the most dramatic or emotional moment in the story
        7. Include facial expressions and emotions from the original text

        Example:
        Input: "ä»–å¸¸å¤¢è¦‹å‰ä»»çµå©šäº†ï¼Œæ¯æ¬¡éƒ½ç¬‘è‘—ç¥ç¦ï¼Œç„¶å¾Œå“­è‘—é†’ä¾†"
        Better output: "Asian man dreaming, wedding scene, forced smile, tears, emotional pain, bittersweet expression, dream-like atmosphere, cinematic lighting, detailed"

        Always capture the EMOTIONAL CORE and most visual elements."""
        user_prompt = f"Story: {dream_text}\nConvert to image prompt:"
        
        raw_prompt = self._call_ollama(system_prompt, user_prompt, temperature=0.5)
        
        if not raw_prompt:
            return None
        
        # æ¸…ç†ä¸¦è™•ç†æç¤ºè©
        clean_prompt = raw_prompt.strip()
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«äººç‰©å…ƒç´ 
        if ('æˆ‘' in dream_text or 'è‡ªå·±' in dream_text) and \
           not any(word in clean_prompt.lower() for word in ['person', 'human', 'figure']):
            clean_prompt = f" I {clean_prompt}"
        
        final_prompt = f"{clean_prompt}, vibrant colors, detailed, cinematic"
        
        print(f"âœ¨ ç”Ÿæˆåœ–åƒæç¤ºè©: {final_prompt}")
        return final_prompt

    def _generate_image(self, dream_text):
        """ç”Ÿæˆåœ–åƒ"""
        if not self._load_image_model() or self.image_pipe is None:
            print("âŒ åœ–åƒæ¨¡å‹æœªè¼‰å…¥")
            return None
        
        try:
            # ç”Ÿæˆæç¤ºè©
            image_prompt = self._generate_image_prompt(dream_text)
            if not image_prompt:
                print("âŒ ç„¡æ³•ç”Ÿæˆåœ–åƒæç¤ºè©")
                return None
            
            # è¨­å®šè² é¢æç¤ºè©
            person_keywords = ['æˆ‘', 'äºº', 'è‡ªå·±', 'å¤¢è¦‹æˆ‘']
            if any(keyword in dream_text for keyword in person_keywords):
                negative_prompt = "ugly, blurry, distorted, deformed, low quality, bad anatomy, multiple heads, extra limbs"
            else:
                negative_prompt = "human, person, ugly, blurry, distorted, deformed, low quality, bad anatomy"
            
            # ç”Ÿæˆåƒæ•¸
            seed = random.randint(0, 2**32 - 1)
            generator = torch.Generator(self.current_device).manual_seed(seed)
            
            generation_params = {
                "prompt": image_prompt,
                "negative_prompt": negative_prompt,
                "height": 512,
                "width": 512,
                "num_inference_steps": 20 if self.current_device != "cpu" else 15,
                "guidance_scale": 7.5,
                "generator": generator
            }
            
            # æ¸…ç†è¨˜æ†¶é«”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # ç”Ÿæˆåœ–åƒ
            print("ğŸ¨ é–‹å§‹ç”Ÿæˆåœ–åƒ...")
            with torch.no_grad():
                result = self.image_pipe(**generation_params)
                
                if not result.images or len(result.images) == 0:
                    print("âŒ åœ–åƒç”Ÿæˆå¤±æ•—")
                    return None
                
                generated_image = result.images[0]
            
            # è™•ç†åœ–åƒ
            if generated_image.mode != 'RGB':
                generated_image = generated_image.convert('RGB')
            
            # èª¿æ•´äº®åº¦
            img_array = np.array(generated_image)
            avg_brightness = np.mean(img_array)
            
            if avg_brightness < 30:
                img_array = np.clip(img_array * 1.3 + 20, 0, 255).astype(np.uint8)
                generated_image = Image.fromarray(img_array)
            
            # ä¿å­˜åœ–åƒ
            timestamp = int(time.time())
            random_id = str(uuid.uuid4())[:8]
            output_filename = f"dream_{timestamp}_{random_id}.png"
            
            output_dir = os.path.join(self.static_dir, 'images')
            output_path = os.path.join(output_dir, output_filename)
            
            generated_image.save(output_path, format='PNG', quality=95)
            print(f"âœ… åœ–åƒå·²ä¿å­˜: {output_filename}")
            
            return os.path.join('images', output_filename)
            
        except Exception as e:
            print(f"âŒ åœ–åƒç”ŸæˆéŒ¯èª¤: {e}")
            return None

    def _generate_video(self, image_path, dream_text):
        """ç”Ÿæˆå½±ç‰‡ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
        try:
            # åˆå§‹åŒ–å½±ç‰‡ç”Ÿæˆå™¨
            video_gen = self._initialize_video_generator()
            
            # ç”Ÿæˆå½±ç‰‡
            video_path = video_gen.generate_video_from_image(
                image_path, self.static_dir, dream_text
            )
            
            return video_path
            
        except Exception as e:
            print(f"âŒ å½±ç‰‡ç”ŸæˆéŒ¯èª¤: {e}")
            return None

    def _analyze_psychology(self, dream_text):
        """å¿ƒç†åˆ†æ"""
        system_prompt = """ä½ æ˜¯å¤¢å¢ƒå¿ƒç†åˆ†æå°ˆå®¶ï¼Œåˆ†æå¤¢å¢ƒçš„è±¡å¾µæ„ç¾©å’Œå¿ƒç†ç‹€æ…‹ï¼Œ
æä¾›150-200å­—çš„åˆ†æï¼Œä½¿ç”¨æº«å’Œæ”¯æŒæ€§èªèª¿ï¼Œç¹é«”ä¸­æ–‡å›ç­”ã€‚"""
        
        user_prompt = f"å¤¢å¢ƒæè¿°: {dream_text}\nè«‹æä¾›å¿ƒç†åˆ†æï¼š"
        
        analysis = self._call_ollama(system_prompt, user_prompt)
        return analysis if analysis else "æš«æ™‚ç„¡æ³•é€²è¡Œå¿ƒç†åˆ†æã€‚"

    def _save_dream_result(self, data):
        """ä¿å­˜å¤¢å¢ƒåˆ†æçµæœ"""
        try:
            share_id = str(uuid.uuid4())
            share_dir = os.path.join(self.static_dir, 'shares')
            
            share_data = {
                'id': share_id,
                'timestamp': int(time.time()),
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'finalStory': data.get('finalStory', ''),
                'imagePath': data.get('imagePath', ''),
                'videoPath': data.get('videoPath', ''),  # æ–°å¢å½±ç‰‡è·¯å¾‘
                'psychologyAnalysis': data.get('psychologyAnalysis', '')
            }
            
            share_file = os.path.join(share_dir, f"{share_id}.json")
            with open(share_file, 'w', encoding='utf-8') as f:
                json.dump(share_data, f, ensure_ascii=False, indent=2)
            
            return share_id
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†äº«çµæœå¤±æ•—: {e}")
            return None

    # è·¯ç”±è™•ç†å‡½æ•¸
    def index(self):
        return render_template('index.html')

    def api_status(self):
        ollama_status = self._check_ollama_status()
        local_models_status = self.models_loaded or self._load_image_model()
        
        # æª¢æŸ¥å½±ç‰‡ç”Ÿæˆå™¨ç‹€æ…‹
        video_status = False
        if self.video_generator:
            video_status = self.video_generator.video_loaded
        
        return jsonify({
            'ollama': ollama_status,
            'local_models': local_models_status,
            'video_models': video_status,  # æ–°å¢å½±ç‰‡æ¨¡å‹ç‹€æ…‹
            'device': self.current_device,
            'available_models': list(self.models.keys()),
            'timestamp': int(time.time())
        })

    def analyze(self):
        data = request.json
        dream_text = data.get('dream', '')
        selected_model = data.get('model', 'stable-diffusion-v1-5')
        generate_video = data.get('generateVideo', False)  # æ–°å¢å½±ç‰‡ç”Ÿæˆé¸é …
        
        # è¼¸å…¥é©—è­‰
        if not dream_text or len(dream_text.strip()) < 10:
            return jsonify({'error': 'è«‹è¼¸å…¥è‡³å°‘10å€‹å­—çš„å¤¢å¢ƒæè¿°'}), 400
        
        if len(dream_text.strip()) > 2000:
            return jsonify({'error': 'å¤¢å¢ƒæè¿°éé•·ï¼Œè«‹æ§åˆ¶åœ¨2000å­—ä»¥å…§'}), 400
        
        # é˜²é‡è¤‡æäº¤æª¢æŸ¥
        request_id = f"{dream_text[:50]}_{int(time.time())}"
        
        if self.request_lock:
            print("âš ï¸  æœ‰å…¶ä»–è«‹æ±‚æ­£åœ¨è™•ç†ä¸­ï¼Œè«‹ç¨å€™...")
            return jsonify({'error': 'ç³»çµ±æ­£åœ¨è™•ç†å…¶ä»–è«‹æ±‚ï¼Œè«‹ç¨å€™å†è©¦'}), 429
        
        if request_id in self.processing_requests:
            print("âš ï¸  ç›¸åŒè«‹æ±‚å·²åœ¨è™•ç†ä¸­...")
            return jsonify({'error': 'ç›¸åŒçš„è«‹æ±‚æ­£åœ¨è™•ç†ä¸­'}), 429
        
        # è¨­å®šè™•ç†ç‹€æ…‹
        self.request_lock = True
        self.processing_requests.add(request_id)
        
        try:
            print(f"ğŸŒ™ é–‹å§‹åˆ†æå¤¢å¢ƒ [ID: {request_id[:20]}...]: {dream_text[:50]}...")
            
            # æª¢æŸ¥æœå‹™ç‹€æ…‹
            ollama_status = self._check_ollama_status()
            if not ollama_status:
                return jsonify({'error': 'Ollamaæœå‹™ä¸å¯ç”¨'}), 503
            
            # ç”Ÿæˆæ•…äº‹
            print("ğŸ“– ç”Ÿæˆå¤¢å¢ƒæ•…äº‹...")
            final_story = self._generate_story(dream_text)
            
            # ç”Ÿæˆåœ–åƒ
            print("ğŸ¨ ç”Ÿæˆå¤¢å¢ƒåœ–åƒ...")
            local_models_status = self._load_image_model()
            image_path = None
            if local_models_status:
                image_path = self._generate_image(dream_text)
            
            # ç”Ÿæˆå½±ç‰‡ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
            video_path = None
            if generate_video and image_path and local_models_status:
                print("ğŸ¬ ç”Ÿæˆå¤¢å¢ƒå½±ç‰‡...")
                video_path = self._generate_video(image_path, dream_text)
                if video_path:
                    print("âœ… å½±ç‰‡ç”ŸæˆæˆåŠŸ")
                else:
                    print("âš ï¸  å½±ç‰‡ç”Ÿæˆå¤±æ•—ï¼Œä½†ä¸å½±éŸ¿å…¶ä»–åŠŸèƒ½")
            
            # å¿ƒç†åˆ†æ
            print("ğŸ§  é€²è¡Œå¿ƒç†åˆ†æ...")
            psychology_analysis = self._analyze_psychology(dream_text)
            
            response = {
                'finalStory': final_story,
                'imagePath': '/static/' + image_path if image_path else None,
                'videoPath': '/static/' + video_path if video_path else None,  # æ–°å¢å½±ç‰‡è·¯å¾‘
                'psychologyAnalysis': psychology_analysis,
                'apiStatus': {
                    'ollama': ollama_status,
                    'local_models': local_models_status,
                    'video_models': self.video_generator.video_loaded if self.video_generator else False,  # æ–°å¢å½±ç‰‡æ¨¡å‹ç‹€æ…‹
                    'device': self.current_device,
                    'current_model': selected_model
                },
                'processingInfo': {
                    'timestamp': int(time.time()),
                    'inputLength': len(dream_text),
                    'storyLength': len(final_story) if final_story else 0,
                    'requestId': request_id[:20],
                    'videoGenerated': video_path is not None  # æ–°å¢å½±ç‰‡ç”Ÿæˆç‹€æ…‹
                }
            }
            
            print(f"âœ… å¤¢å¢ƒåˆ†æå®Œæˆ [ID: {request_id[:20]}...]")
            return jsonify(response)
            
        except Exception as e:
            print(f"âŒ åˆ†æéŒ¯èª¤ [ID: {request_id[:20]}...]: {e}")
            return jsonify({'error': 'è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤'}), 500
        
        finally:
            # æ¸…ç†è™•ç†ç‹€æ…‹
            self.request_lock = False
            self.processing_requests.discard(request_id)
            print(f"ğŸ”“ é‡‹æ”¾è«‹æ±‚é– [ID: {request_id[:20]}...]")

    def share_result(self):
        data = request.json
        
        if not data or 'finalStory' not in data:
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦çš„å¤¢å¢ƒåˆ†ææ•¸æ“š'}), 400
        
        try:
            share_id = self._save_dream_result(data)
            
            if not share_id:
                return jsonify({'error': 'å‰µå»ºåˆ†äº«å¤±æ•—'}), 500
            
            share_url = url_for('view_shared', share_id=share_id, _external=True)
            
            return jsonify({
                'shareId': share_id, 
                'shareUrl': share_url,
                'timestamp': int(time.time())
            })
            
        except Exception as e:
            print(f"âŒ åˆ†äº«è™•ç†éŒ¯èª¤: {e}")
            return jsonify({'error': 'è™•ç†åˆ†äº«è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤'}), 500

    def view_shared(self, share_id):
        try:
            share_file = os.path.join(self.static_dir, 'shares', f"{share_id}.json")
            
            if not os.path.exists(share_file):
                return jsonify({'error': 'æ‰¾ä¸åˆ°è©²åˆ†äº«å…§å®¹'}), 404
            
            with open(share_file, 'r', encoding='utf-8') as f:
                share_data = json.load(f)
            
            return render_template('shared.html', data=share_data)
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥åˆ†äº«å…§å®¹éŒ¯èª¤: {e}")
            return jsonify({'error': 'è¼‰å…¥åˆ†äº«å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤'}), 500

    def clear_all_memory(self):
        """æ¸…ç†æ‰€æœ‰æ¨¡å‹è¨˜æ†¶é«”"""
        # æ¸…ç†åœ–åƒæ¨¡å‹
        if self.image_pipe is not None:
            del self.image_pipe
            self.image_pipe = None
        
        # æ¸…ç†å½±ç‰‡æ¨¡å‹
        if self.video_generator:
            self.video_generator.clear_video_memory()
        
        self.models_loaded = False
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        print("ğŸ§¹ æ‰€æœ‰æ¨¡å‹è¨˜æ†¶é«”å·²æ¸…ç†")

    def run(self, debug=False, host='0.0.0.0', port=5002):
        """å•Ÿå‹•æ‡‰ç”¨"""
        try:
            # åœ¨ç¨‹å¼çµæŸæ™‚æ¸…ç†è¨˜æ†¶é«”
            import atexit
            atexit.register(self.clear_all_memory)
            
            print("ğŸš€ å•Ÿå‹•å¤¢å¢ƒåˆ†ææ‡‰ç”¨...")
            
            # æª¢æŸ¥æœå‹™ç‹€æ…‹
            print("æª¢æŸ¥æœå‹™ç‹€æ…‹...")
            ollama_status = self._check_ollama_status()
            local_models_status = self._load_image_model()
            
            # è¼¸å‡ºç‹€æ…‹å ±å‘Š
            print("=" * 80)
            print("å¤¢å¢ƒåˆ†æç³»çµ± - å«å½±ç‰‡ç”ŸæˆåŠŸèƒ½ å•Ÿå‹•ç‹€æ…‹å ±å‘Š")
            print("=" * 80)
            print(f"Ollama API (localhost:11434): {'âœ… æ­£å¸¸' if ollama_status else 'âŒ ç•°å¸¸'}")
            print(f"æœ¬åœ°åœ–åƒç”Ÿæˆæ¨¡å‹: {'âœ… å¯ç”¨' if local_models_status else 'âŒ ä¸å¯ç”¨'}")
            print(f"æœ¬åœ°å½±ç‰‡ç”Ÿæˆæ¨¡å‹: {'âœ… å¯ç”¨' if self.video_generator and self.video_generator.video_loaded else 'âš ï¸  éœ€è¦æ™‚è¼‰å…¥'}")
            print(f"éœæ…‹æª”æ¡ˆç›®éŒ„: {self.static_dir}")
            
            # æª¢æŸ¥ PyTorch å’Œè¨­å‚™æ”¯æŒ
            if torch.backends.mps.is_available():
                print("âœ… å·²å•Ÿç”¨ Metal Performance Shaders (MPS) åŠ é€Ÿ")
                device_info = "MPS (Apple Silicon å„ªåŒ–)"
            elif torch.cuda.is_available():
                print("âœ… å·²å•Ÿç”¨ CUDA åŠ é€Ÿ")
                device_info = f"CUDA - {torch.cuda.get_device_name()}"
            else:
                print("âš ï¸  ä½¿ç”¨ CPU æ¨¡å¼ï¼Œé€Ÿåº¦å¯èƒ½è¼ƒæ…¢")
                device_info = "CPU"
            
            print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
            print(f"ä½¿ç”¨è¨­å‚™: {device_info}")
            print("=" * 80)
            
            # ç³»çµ±åŠŸèƒ½èªªæ˜
            print("ğŸ”§ ç³»çµ±åŠŸèƒ½ç‹€æ…‹:")
            print(f"   â€¢ æ•…äº‹ç”Ÿæˆ: {'âœ… å¯ç”¨ (Ollama qwen2.5:14b)' if ollama_status else 'âŒ ä¸å¯ç”¨'}")
            print(f"   â€¢ åœ–åƒç”Ÿæˆ: {'âœ… å¯ç”¨ (Stable Diffusion v1.5)' if local_models_status else 'âŒ ä¸å¯ç”¨'}")
            print(f"   â€¢ å½±ç‰‡ç”Ÿæˆ: {'âœ… å¯ç”¨ (Stable Video Diffusion)' if self.video_generator else 'âš ï¸  éœ€è¦æ™‚è¼‰å…¥'}")
            print(f"   â€¢ å¿ƒç†åˆ†æ: {'âœ… å¯ç”¨ (Ollama)' if ollama_status else 'âŒ ä¸å¯ç”¨'}")
            print()
            
            print("ğŸ¬ å½±ç‰‡ç”Ÿæˆç‰¹æ€§:")
            print("   â€¢ åœ–åƒè½‰å½±ç‰‡: å¾ç”Ÿæˆçš„åœ–åƒå‰µå»ºå‹•æ…‹å½±ç‰‡")
            print("   â€¢ è§£æåº¦: 1024x576ï¼Œ25å¹€ (CPUæ¨¡å¼ç‚º14å¹€)")
            print("   â€¢ å¹€ç‡: 8 FPS")
            print("   â€¢ æ ¼å¼: MP4")
            print("   â€¢ æ™ºèƒ½è¨˜æ†¶é«”ç®¡ç†")
            print("   â€¢ å¯é¸æ“‡æ˜¯å¦ç”Ÿæˆå½±ç‰‡")
            print()
            
            if not ollama_status:
                print("âŒ è­¦å‘Š: Ollama API ç„¡æ³•é€£æ¥")
                print("   è«‹ç¢ºèª Ollama æœå‹™æ˜¯å¦é‹è¡Œåœ¨ localhost:11434")
                print("   å•Ÿå‹•å‘½ä»¤: ollama serve")
                print("   å¿…é ˆå…ˆå®‰è£æ¨¡å‹: ollama pull qwen2.5:14b")
                print()
            
            if not local_models_status:
                print("âŒ è­¦å‘Š: æœ¬åœ°ç”Ÿæˆæ¨¡å‹ä¸å¯ç”¨")
                print("   é¦–æ¬¡é‹è¡Œæ™‚æœƒè‡ªå‹•ä¸‹è¼‰æ¨¡å‹")
                print("   åœ–åƒæ¨¡å‹ç´„ 4GBï¼Œå½±ç‰‡æ¨¡å‹ç´„ 6-8GB")
                print("   è«‹ç¢ºä¿ç¶²è·¯é€£æ¥æ­£å¸¸ä¸”æœ‰è¶³å¤ çš„å„²å­˜ç©ºé–“")
                print()
            
            print("âš¡ æ€§èƒ½é æœŸ:")
            if device_info.startswith("MPS"):
                print("   â€¢ Apple Silicon å„ªåŒ–")
                print("   â€¢ åœ–åƒç”Ÿæˆ: 10-30 ç§’")
                print("   â€¢ å½±ç‰‡ç”Ÿæˆ: 1-3 åˆ†é˜")
                print("   â€¢ å»ºè­°: 16GB+ çµ±ä¸€è¨˜æ†¶é«”")
            elif device_info.startswith("CUDA"):
                print("   â€¢ GPU åŠ é€Ÿï¼Œé€Ÿåº¦æœ€å¿«")
                print("   â€¢ åœ–åƒç”Ÿæˆ: 5-15 ç§’")
                print("   â€¢ å½±ç‰‡ç”Ÿæˆ: 30ç§’-2åˆ†é˜")
                print("   â€¢ å»ºè­°: 8GB+ VRAM")
            else:
                print("   â€¢ CPU æ¨¡å¼ï¼Œé€Ÿåº¦è¼ƒæ…¢")
                print("   â€¢ åœ–åƒç”Ÿæˆ: 1-3 åˆ†é˜")
                print("   â€¢ å½±ç‰‡ç”Ÿæˆ: 5-10 åˆ†é˜")
                print("   â€¢ å»ºè­°: 16GB+ RAM")
            print()
            
            print("ğŸ†• æ–°å¢åŠŸèƒ½:")
            print("   â€¢ ç¨ç«‹çš„å½±ç‰‡ç”Ÿæˆæ¨¡çµ„")
            print("   â€¢ å¯é¸æ“‡æ˜¯å¦ç”Ÿæˆå½±ç‰‡")
            print("   â€¢ å½±ç‰‡ç”Ÿæˆç‹€æ…‹ç›£æ§")
            print("   â€¢ æ™ºèƒ½è¨˜æ†¶é«”æ¸…ç†")
            print("   â€¢ åˆ†äº«åŠŸèƒ½åŒ…å«å½±ç‰‡")
            print()
            
            print("=" * 80)
            print("ç³»çµ±æº–å‚™å°±ç·’ï¼Œå•Ÿå‹• Flask æ‡‰ç”¨ç¨‹å¼...")
            print("è¨ªå•åœ°å€: http://localhost:5002")
            print("=" * 80)
            
            self.app.run(debug=debug, host=host, port=port, threaded=True)
            
        except KeyboardInterrupt:
            print("ç”¨æˆ¶ä¸­æ–·ï¼Œæ­£åœ¨é—œé–‰ç³»çµ±...")
            self.clear_all_memory()
        except Exception as e:
            print(f"ç³»çµ±å•Ÿå‹•å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            self.clear_all_memory()
        finally:
            print("æ­£åœ¨æ¸…ç†ç³»çµ±è³‡æº...")
            self.clear_all_memory()


# ä¸»ç¨‹å¼å…¥å£
if __name__ == '__main__':
    dream_analyzer = DreamAnalyzer()
    dream_analyzer.run(debug=False)
