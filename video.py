import os
import time
import uuid
import json
import random
from flask import Flask, request, jsonify, render_template, url_for
import requests
from PIL import Image
import numpy as np
import torch
from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler
import gc

# æ–°å¢ï¼šå°å…¥æœ¬åœ°è¦–é »ç”Ÿæˆç›¸é—œå¥—ä»¶
import cv2
from moviepy.editor import ImageClip, concatenate_videoclips
from scipy.ndimage import gaussian_filter
import tempfile

class DreamAnalyzer:
    def __init__(self):
        self.app_root = os.path.dirname(os.path.abspath(__file__))
        self.static_dir = os.path.join(self.app_root, 'static')
        self.app = Flask(__name__, 
                        template_folder=os.path.join(self.app_root, 'templates'),
                        static_folder=self.static_dir)
        
        # é…ç½®
        self.OLLAMA_API = "http://localhost:11434/api/generate"
        self.OLLAMA_MODEL = "qwen2.5:14b"
        
        # æ¨¡å‹ç‹€æ…‹
        self.image_pipe = None
        self.models_loaded = False
        self.current_device = None
        self.torch_dtype = None
        
        # å¯ç”¨æ¨¡å‹
        self.models = {
            "stable-diffusion-v1-5": "runwayml/stable-diffusion-v1-5",
        }
        
        # é˜²é‡è¤‡æäº¤
        self.processing_requests = set()  # æ­£åœ¨è™•ç†çš„è«‹æ±‚ID
        self.request_lock = False  # å…¨å±€é–
        
        self._setup_routes()
        self._create_directories()

    def _create_directories(self):
        """å‰µå»ºå¿…è¦çš„ç›®éŒ„"""
        directories = [
            os.path.join(self.static_dir, 'images'),
            os.path.join(self.static_dir, 'videos'),  # æ–°å¢ï¼šè¦–é »ç›®éŒ„
            os.path.join(self.static_dir, 'shares')
        ]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)

    def _setup_routes(self):
        """è¨­å®šè·¯ç”±"""
        self.app.route('/')(self.index)
        self.app.route('/api/status')(self.api_status)
        self.app.route('/api/analyze', methods=['POST'])(self.analyze)
        self.app.route('/api/share', methods=['POST'])(self.share_result)
        self.app.route('/share/<share_id>')(self.view_shared)

    def _initialize_device(self):
        """åˆå§‹åŒ–è¨­å‚™è¨­å®š"""
        if torch.cuda.is_available():
            self.current_device = "cuda"
            self.torch_dtype = torch.float16
        elif torch.backends.mps.is_available():
            self.current_device = "mps"
            self.torch_dtype = torch.float32
        else:
            self.current_device = "cpu"
            self.torch_dtype = torch.float32

    def _load_image_model(self):
        """è¼‰å…¥åœ–åƒç”Ÿæˆæ¨¡å‹"""
        if self.models_loaded:
            return True
        
        try:
            self._initialize_device()
            model_id = "runwayml/stable-diffusion-v1-5"
            
            self.image_pipe = StableDiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=self.torch_dtype,
                use_safetensors=True,
                safety_checker=None,
                requires_safety_checker=False
            ).to(self.current_device)
            
            self.image_pipe.scheduler = UniPCMultistepScheduler.from_config(
                self.image_pipe.scheduler.config
            )
            
            # å„ªåŒ–è¨­å®š
            if self.current_device == "cuda":
                self.image_pipe.enable_model_cpu_offload()
                self.image_pipe.enable_attention_slicing()
                self.image_pipe.enable_vae_slicing()
            elif self.current_device == "mps":
                self.image_pipe.enable_attention_slicing(1)
            else:
                self.image_pipe.enable_attention_slicing()
            
            self.models_loaded = True
            print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨è¨­å‚™: {self.current_device}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return False

    def _check_ollama_status(self):
        """æª¢æŸ¥ Ollama æœå‹™ç‹€æ…‹"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False

    # æ–°å¢ï¼šæª¢æŸ¥æœ¬åœ°è¦–é »ç”ŸæˆåŠŸèƒ½ç‹€æ…‹
    def _check_local_video_status(self):
        """æª¢æŸ¥æœ¬åœ°è¦–é »ç”ŸæˆåŠŸèƒ½ç‹€æ…‹"""
        try:
            # æª¢æŸ¥å¿…è¦çš„å¥—ä»¶æ˜¯å¦å¯ç”¨
            import cv2
            import moviepy
            return True
        except ImportError:
            return False

    def _call_ollama(self, system_prompt, user_prompt, temperature=0.7):
        """èª¿ç”¨ Ollama API"""
        try:
            data = {
                "model": self.OLLAMA_MODEL,
                "prompt": user_prompt,
                "system": system_prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "top_p": 0.9,
                    "num_predict": 500
                }
            }
            
            response = requests.post(self.OLLAMA_API, json=data, timeout=120)
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            return ""
        except Exception as e:
            print(f"âŒ Ollama èª¿ç”¨å¤±æ•—: {e}")
            return ""

    def _generate_story(self, dream_text):
        """ç”Ÿæˆå¤¢å¢ƒæ•…äº‹"""
        system_prompt = """ä½ æ˜¯å¤¢å¢ƒæ•…äº‹å‰µä½œå°ˆå®¶ï¼Œè¦æ±‚ï¼š
1. ç›´æ¥é–‹å§‹æ•…äº‹ï¼Œç„¡å•å€™èª
2. èåˆå¤¢å¢ƒå…ƒç´ 
3. ä½¿ç”¨ç¬¬ä¸€äººç¨±
4. 150-200å­—å®Œæ•´æ•…äº‹
5. ç¹é«”ä¸­æ–‡"""

        user_prompt = f"åŸºæ–¼å¤¢å¢ƒç‰‡æ®µå‰µä½œæ•…äº‹ï¼šã€Œ{dream_text}ã€"
        
        story = self._call_ollama(system_prompt, user_prompt)
        return self._clean_story_content(story) if story else "ç„¡æ³•ç”Ÿæˆå¤¢å¢ƒæ•…äº‹"

    def _clean_story_content(self, story):
        """æ¸…ç†æ•…äº‹å…§å®¹"""
        unwanted_phrases = [
            "å¥½çš„ï¼Œæ ¹æ“šæ‚¨çš„å»ºè­°", "æ ¹æ“šæ‚¨çš„è¦æ±‚", "ä»¥ä¸‹æ˜¯æ•…äº‹", "æ•…äº‹å¦‚ä¸‹",
            "###", "**", "æ•…äº‹åç¨±ï¼š", "å¤¢å¢ƒæ•…äº‹ï¼š", "å®Œæ•´æ•…äº‹ï¼š"
        ]
        
        cleaned_story = story.strip()
        
        for phrase in unwanted_phrases:
            if cleaned_story.startswith(phrase):
                cleaned_story = cleaned_story[len(phrase):].strip()
            if phrase in cleaned_story:
                parts = cleaned_story.split(phrase)
                if len(parts) > 1:
                    cleaned_story = parts[-1].strip()
        
        # ç§»é™¤å¼•è™Ÿ
        if cleaned_story.startswith('"') and cleaned_story.endswith('"'):
            cleaned_story = cleaned_story[1:-1].strip()
        if cleaned_story.startswith('ã€Œ') and cleaned_story.endswith('ã€'):
            cleaned_story = cleaned_story[1:-1].strip()
        
        return cleaned_story.replace('*', '').replace('#', '').strip()

    def _generate_image_prompt(self, dream_text):
        """å°‡å¤¢å¢ƒè½‰æ›ç‚ºåœ–åƒç”Ÿæˆæç¤ºè©"""
        system_prompt = """You are a Stable Diffusion prompt expert. Convert user input into English image generation prompts. Requirements:
        1. PRESERVE and translate ALL original elements from the input
        2. If input is a story/emotion, extract the MOST VISUAL and EMOTIONAL scene
        3. ENHANCE with additional quality details, don't replace original content
        4. Use English keywords only
        5. The main characters are mostly Asian
        6. Focus on the most dramatic or emotional moment in the story
        7. Include facial expressions and emotions from the original text

        Example:
        Input: "ä»–å¸¸å¤¢è¦‹å‰ä»»çµå©šäº†ï¼Œæ¯æ¬¡éƒ½ç¬‘è‘—ç¥ç¦ï¼Œç„¶å¾Œå“­è‘—é†’ä¾†"
        Better output: "Asian man dreaming, wedding scene, forced smile, tears, emotional pain, bittersweet expression, dream-like atmosphere, cinematic lighting, detailed"

        Always capture the EMOTIONAL CORE and most visual elements."""
        user_prompt = f"Story: {dream_text}\nConvert to image prompt:"
        
        raw_prompt = self._call_ollama(system_prompt, user_prompt, temperature=0.5)
        
        if not raw_prompt:
            return None
        
        # æ¸…ç†ä¸¦è™•ç†æç¤ºè©
        clean_prompt = raw_prompt.strip()
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«äººç‰©å…ƒç´ 
        if ('æˆ‘' in dream_text or 'è‡ªå·±' in dream_text) and \
           not any(word in clean_prompt.lower() for word in ['person', 'human', 'figure']):
            clean_prompt = f" I {clean_prompt}"
        
        final_prompt = f"{clean_prompt}, vibrant colors, detailed, cinematic"
        
        print(f"âœ¨ ç”Ÿæˆåœ–åƒæç¤ºè©: {final_prompt}")
        return final_prompt

    def _generate_image(self, dream_text):
        """ç”Ÿæˆåœ–åƒ"""
        if not self._load_image_model() or self.image_pipe is None:
            print("âŒ åœ–åƒæ¨¡å‹æœªè¼‰å…¥")
            return None
        
        try:
            # ç”Ÿæˆæç¤ºè©
            image_prompt = self._generate_image_prompt(dream_text)
            if not image_prompt:
                print("âŒ ç„¡æ³•ç”Ÿæˆåœ–åƒæç¤ºè©")
                return None
            
            # è¨­å®šè² é¢æç¤ºè©
            person_keywords = ['æˆ‘', 'äºº', 'è‡ªå·±', 'å¤¢è¦‹æˆ‘']
            if any(keyword in dream_text for keyword in person_keywords):
                negative_prompt = "ugly, blurry, distorted, deformed, low quality, bad anatomy, multiple heads, extra limbs"
            else:
                negative_prompt = "human, person, ugly, blurry, distorted, deformed, low quality, bad anatomy"
            
            # ç”Ÿæˆåƒæ•¸
            seed = random.randint(0, 2**32 - 1)
            generator = torch.Generator(self.current_device).manual_seed(seed)
            
            generation_params = {
                "prompt": image_prompt,
                "negative_prompt": negative_prompt,
                "height": 512,
                "width": 512,
                "num_inference_steps": 20 if self.current_device != "cpu" else 15,
                "guidance_scale": 7.5,
                "generator": generator
            }
            
            # æ¸…ç†è¨˜æ†¶é«”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # ç”Ÿæˆåœ–åƒ
            print("ğŸ¨ é–‹å§‹ç”Ÿæˆåœ–åƒ...")
            with torch.no_grad():
                result = self.image_pipe(**generation_params)
                
                if not result.images or len(result.images) == 0:
                    print("âŒ åœ–åƒç”Ÿæˆå¤±æ•—")
                    return None
                
                generated_image = result.images[0]
            
            # è™•ç†åœ–åƒ
            if generated_image.mode != 'RGB':
                generated_image = generated_image.convert('RGB')
            
            # èª¿æ•´äº®åº¦
            img_array = np.array(generated_image)
            avg_brightness = np.mean(img_array)
            
            if avg_brightness < 30:
                img_array = np.clip(img_array * 1.3 + 20, 0, 255).astype(np.uint8)
                generated_image = Image.fromarray(img_array)
            
            # ä¿å­˜åœ–åƒ
            timestamp = int(time.time())
            random_id = str(uuid.uuid4())[:8]
            output_filename = f"dream_{timestamp}_{random_id}.png"
            
            output_dir = os.path.join(self.static_dir, 'images')
            output_path = os.path.join(output_dir, output_filename)
            
            generated_image.save(output_path, format='PNG', quality=95)
            print(f"âœ… åœ–åƒå·²ä¿å­˜: {output_filename}")
            
            return os.path.join('images', output_filename)
            
        except Exception as e:
            print(f"âŒ åœ–åƒç”ŸæˆéŒ¯èª¤: {e}")
            return None

    # æ–°å¢ï¼šæœ¬åœ°è¼•é‡å‹è¦–é »ç”ŸæˆåŠŸèƒ½
    def _create_parallax_effect(self, image_array, frames=30, zoom_factor=1.1, pan_speed=2):
        """å‰µå»ºè¦–å·®æ•ˆæœ"""
        height, width = image_array.shape[:2]
        center_x, center_y = width // 2, height // 2
        
        effect_frames = []
        
        for i in range(frames):
            # è¨ˆç®—ç¸®æ”¾å’Œå¹³ç§»
            scale = 1.0 + (zoom_factor - 1.0) * (i / frames)
            
            # å¹³ç§»æ•ˆæœï¼ˆè¼•å¾®æ–æ“ºï¼‰
            offset_x = int(pan_speed * np.sin(i * 0.2))
            offset_y = int(pan_speed * 0.5 * np.cos(i * 0.15))
            
            # å‰µå»ºè®Šæ›çŸ©é™£
            M = cv2.getRotationMatrix2D((center_x, center_y), 0, scale)
            M[0, 2] += offset_x
            M[1, 2] += offset_y
            
            # æ‡‰ç”¨è®Šæ›
            transformed = cv2.warpAffine(image_array, M, (width, height), 
                                       borderMode=cv2.BORDER_REFLECT)
            
            effect_frames.append(transformed)
        
        return effect_frames

    def _create_breathing_effect(self, image_array, frames=30, intensity=0.03):
        """å‰µå»ºå‘¼å¸æ•ˆæœ"""
        height, width = image_array.shape[:2]
        center_x, center_y = width // 2, height // 2
        
        effect_frames = []
        
        for i in range(frames):
            # å‘¼å¸ç¸®æ”¾ï¼ˆæ­£å¼¦æ³¢ï¼‰
            scale = 1.0 + intensity * np.sin(i * 2 * np.pi / frames)
            
            # å‰µå»ºè®Šæ›çŸ©é™£
            M = cv2.getRotationMatrix2D((center_x, center_y), 0, scale)
            
            # æ‡‰ç”¨è®Šæ›
            transformed = cv2.warpAffine(image_array, M, (width, height), 
                                       borderMode=cv2.BORDER_REFLECT)
            
            effect_frames.append(transformed)
        
        return effect_frames

    def _create_wave_effect(self, image_array, frames=30, amplitude=3, frequency=0.1):
        """å‰µå»ºæ³¢æµªæ•ˆæœ"""
        height, width = image_array.shape[:2]
        effect_frames = []
        
        for i in range(frames):
            # å‰µå»ºæ³¢æµªè®Šå½¢
            displaced_image = image_array.copy()
            
            for y in range(height):
                # è¨ˆç®—æ°´å¹³ä½ç§»
                offset = int(amplitude * np.sin(frequency * y + i * 0.3))
                if offset != 0:
                    if offset > 0:
                        displaced_image[y, offset:] = image_array[y, :-offset]
                        displaced_image[y, :offset] = image_array[y, -offset:]
                    else:
                        displaced_image[y, :offset] = image_array[y, -offset:]
                        displaced_image[y, offset:] = image_array[y, :-offset]
            
            effect_frames.append(displaced_image)
        
        return effect_frames

    def _generate_video_from_image(self, image_path, video_type="parallax", duration=3.0):
        """ä½¿ç”¨æœ¬åœ°è¼•é‡å‹æ–¹æ³•å¾åœ–åƒç”Ÿæˆè¦–é »"""
        if not self._check_local_video_status():
            print("âŒ æœ¬åœ°è¦–é »ç”ŸæˆåŠŸèƒ½ä¸å¯ç”¨")
            return None
        
        try:
            print("ğŸ¬ é–‹å§‹æœ¬åœ°è¦–é »ç”Ÿæˆ...")
            
            # è®€å–åœ–åƒ
            full_image_path = os.path.join(self.static_dir, image_path)
            if not os.path.exists(full_image_path):
                print(f"âŒ åœ–åƒæ–‡ä»¶ä¸å­˜åœ¨: {full_image_path}")
                return None
            
            # è¼‰å…¥åœ–åƒ
            image = cv2.imread(full_image_path)
            if image is None:
                print("âŒ ç„¡æ³•è¼‰å…¥åœ–åƒ")
                return None
            
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # è¨ˆç®—å¹€æ•¸
            fps = 10  # é™ä½å¹€ç‡ä»¥æé«˜ç”Ÿæˆé€Ÿåº¦
            total_frames = int(duration * fps)
            
            # æ ¹æ“šé¡å‹ç”Ÿæˆä¸åŒæ•ˆæœ
            print(f"ğŸ¨ ç”Ÿæˆ {video_type} æ•ˆæœ...")
            
            if video_type == "parallax":
                frames = self._create_parallax_effect(image_rgb, total_frames)
            elif video_type == "breathing":
                frames = self._create_breathing_effect(image_rgb, total_frames)
            elif video_type == "wave":
                frames = self._create_wave_effect(image_rgb, total_frames)
            else:
                # é»˜èªçµ„åˆæ•ˆæœ
                frames1 = self._create_parallax_effect(image_rgb, total_frames // 2)
                frames2 = self._create_breathing_effect(image_rgb, total_frames // 2)
                frames = frames1 + frames2
            
            # æ·»åŠ æ·¡å…¥æ·¡å‡ºæ•ˆæœ
            for i in range(min(5, len(frames))):
                alpha = i / 5.0
                frames[i] = (frames[i] * alpha + image_rgb * (1 - alpha)).astype(np.uint8)
            
            for i in range(max(0, len(frames) - 5), len(frames)):
                alpha = (len(frames) - 1 - i) / 5.0
                frames[i] = (frames[i] * alpha + image_rgb * (1 - alpha)).astype(np.uint8)
            
            # ä¿å­˜ç‚ºè¦–é »
            timestamp = int(time.time())
            random_id = str(uuid.uuid4())[:8]
            video_filename = f"dream_video_{timestamp}_{random_id}.mp4"
            
            video_dir = os.path.join(self.static_dir, 'videos')
            video_path = os.path.join(video_dir, video_filename)
            
            # ä½¿ç”¨ moviepy å‰µå»ºè¦–é »
            print("ğŸ’¾ ä¿å­˜è¦–é »æ–‡ä»¶...")
            
            # è½‰æ›å¹€æ ¼å¼
            pil_frames = [Image.fromarray(frame) for frame in frames]
            
            # å‰µå»ºè‡¨æ™‚æ–‡ä»¶ä¾†å­˜å„²å¹€
            temp_dir = tempfile.mkdtemp()
            temp_files = []
            
            try:
                for i, frame in enumerate(pil_frames):
                    temp_file = os.path.join(temp_dir, f"frame_{i:04d}.png")
                    frame.save(temp_file)
                    temp_files.append(temp_file)
                
                # ä½¿ç”¨ ImageClip å‰µå»ºè¦–é »
                clips = []
                frame_duration = duration / len(temp_files)
                
                for temp_file in temp_files:
                    clip = ImageClip(temp_file, duration=frame_duration)
                    clips.append(clip)
                
                final_video = concatenate_videoclips(clips, method="compose")
                
                # å¯«å…¥è¦–é »æ–‡ä»¶
                final_video.write_videofile(
                    video_path,
                    fps=fps,
                    codec='libx264',
                    audio=False,
                    verbose=False,
                    logger=None
                )
                
                # æ¸…ç†è³‡æº
                final_video.close()
                
                print(f"âœ… è¦–é »å·²ä¿å­˜: {video_filename}")
                return os.path.join('videos', video_filename)
            
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                for temp_file in temp_files:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                if os.path.exists(temp_dir):
                    os.rmdir(temp_dir)
            
        except Exception as e:
            print(f"âŒ è¦–é »ç”ŸæˆéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _analyze_psychology(self, dream_text):
        """å¿ƒç†åˆ†æ"""
        system_prompt = """ä½ æ˜¯å¤¢å¢ƒå¿ƒç†åˆ†æå°ˆå®¶ï¼Œåˆ†æå¤¢å¢ƒçš„è±¡å¾µæ„ç¾©å’Œå¿ƒç†ç‹€æ…‹ï¼Œ
æä¾›150-200å­—çš„åˆ†æï¼Œä½¿ç”¨æº«å’Œæ”¯æŒæ€§èªèª¿ï¼Œç¹é«”ä¸­æ–‡å›ç­”ã€‚"""
        
        user_prompt = f"å¤¢å¢ƒæè¿°: {dream_text}\nè«‹æä¾›å¿ƒç†åˆ†æï¼š"
        
        analysis = self._call_ollama(system_prompt, user_prompt)
        return analysis if analysis else "æš«æ™‚ç„¡æ³•é€²è¡Œå¿ƒç†åˆ†æã€‚"

    def _save_dream_result(self, data):
        """ä¿å­˜å¤¢å¢ƒåˆ†æçµæœ"""
        try:
            share_id = str(uuid.uuid4())
            share_dir = os.path.join(self.static_dir, 'shares')
            
            share_data = {
                'id': share_id,
                'timestamp': int(time.time()),
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'finalStory': data.get('finalStory', ''),
                'imagePath': data.get('imagePath', ''),
                'videoPath': data.get('videoPath', ''),  # æ–°å¢ï¼šè¦–é »è·¯å¾‘
                'psychologyAnalysis': data.get('psychologyAnalysis', '')
            }
            
            share_file = os.path.join(share_dir, f"{share_id}.json")
            with open(share_file, 'w', encoding='utf-8') as f:
                json.dump(share_data, f, ensure_ascii=False, indent=2)
            
            return share_id
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†äº«çµæœå¤±æ•—: {e}")
            return None

    # è·¯ç”±è™•ç†å‡½æ•¸
    def index(self):
        return render_template('index.html')

    def api_status(self):
        ollama_status = self._check_ollama_status()
        local_models_status = self.models_loaded or self._load_image_model()
        local_video_status = self._check_local_video_status()  # æ–°å¢ï¼šæª¢æŸ¥æœ¬åœ°è¦–é »ç”Ÿæˆç‹€æ…‹
        
        return jsonify({
            'ollama': ollama_status,
            'local_models': local_models_status,
            'local_video': local_video_status,  # æ–°å¢ï¼šè¿”å›æœ¬åœ°è¦–é »ç”Ÿæˆç‹€æ…‹
            'device': self.current_device,
            'available_models': list(self.models.keys()),
            'timestamp': int(time.time())
        })

    def analyze(self):
        data = request.json
        dream_text = data.get('dream', '')
        selected_model = data.get('model', 'stable-diffusion-v1-5')
        generate_video = data.get('generateVideo', False)  # æ–°å¢ï¼šè¦–é »ç”Ÿæˆé¸é …
        video_type = data.get('videoType', 'parallax')  # æ–°å¢ï¼šè¦–é »é¡å‹é¸é …
        
        # è¼¸å…¥é©—è­‰
        if not dream_text or len(dream_text.strip()) < 10:
            return jsonify({'error': 'è«‹è¼¸å…¥è‡³å°‘10å€‹å­—çš„å¤¢å¢ƒæè¿°'}), 400
        
        if len(dream_text.strip()) > 2000:
            return jsonify({'error': 'å¤¢å¢ƒæè¿°éé•·ï¼Œè«‹æ§åˆ¶åœ¨2000å­—ä»¥å…§'}), 400
        
        # é˜²é‡è¤‡æäº¤æª¢æŸ¥
        request_id = f"{dream_text[:50]}_{int(time.time())}"
        
        if self.request_lock:
            print("âš ï¸  æœ‰å…¶ä»–è«‹æ±‚æ­£åœ¨è™•ç†ä¸­ï¼Œè«‹ç¨å€™...")
            return jsonify({'error': 'ç³»çµ±æ­£åœ¨è™•ç†å…¶ä»–è«‹æ±‚ï¼Œè«‹ç¨å€™å†è©¦'}), 429
        
        if request_id in self.processing_requests:
            print("âš ï¸  ç›¸åŒè«‹æ±‚å·²åœ¨è™•ç†ä¸­...")
            return jsonify({'error': 'ç›¸åŒçš„è«‹æ±‚æ­£åœ¨è™•ç†ä¸­'}), 429
        
        # è¨­å®šè™•ç†ç‹€æ…‹
        self.request_lock = True
        self.processing_requests.add(request_id)
        
        try:
            print(f"ğŸŒ™ é–‹å§‹åˆ†æå¤¢å¢ƒ [ID: {request_id[:20]}...]: {dream_text[:50]}...")
            
            # æª¢æŸ¥æœå‹™ç‹€æ…‹
            ollama_status = self._check_ollama_status()
            if not ollama_status:
                return jsonify({'error': 'Ollamaæœå‹™ä¸å¯ç”¨'}), 503
            
            # ç”Ÿæˆæ•…äº‹
            print("ğŸ“– ç”Ÿæˆå¤¢å¢ƒæ•…äº‹...")
            final_story = self._generate_story(dream_text)
            
            # ç”Ÿæˆåœ–åƒ
            print("ğŸ¨ ç”Ÿæˆå¤¢å¢ƒåœ–åƒ...")
            local_models_status = self._load_image_model()
            image_path = None
            if local_models_status:
                image_path = self._generate_image(dream_text)
            
            # æ–°å¢ï¼šç”Ÿæˆè¦–é »ï¼ˆå¦‚æœè¦æ±‚ä¸”åœ–åƒç”ŸæˆæˆåŠŸï¼‰
            video_path = None
            if generate_video and image_path and self._check_local_video_status():
                print("ğŸ¬ ç”Ÿæˆå¤¢å¢ƒè¦–é »...")
                video_path = self._generate_video_from_image(image_path, video_type)
            
            # å¿ƒç†åˆ†æ
            print("ğŸ§  é€²è¡Œå¿ƒç†åˆ†æ...")
            psychology_analysis = self._analyze_psychology(dream_text)
            
            response = {
                'finalStory': final_story,
                'imagePath': '/static/' + image_path if image_path else None,
                'videoPath': '/static/' + video_path if video_path else None,  # æ–°å¢ï¼šè¦–é »è·¯å¾‘
                'psychologyAnalysis': psychology_analysis,
                'apiStatus': {
                    'ollama': ollama_status,
                    'local_models': local_models_status,
                    'local_video': self._check_local_video_status(),  # æ–°å¢ï¼šæœ¬åœ°è¦–é »æœå‹™ç‹€æ…‹
                    'device': self.current_device,
                    'current_model': selected_model
                },
                'processingInfo': {
                    'timestamp': int(time.time()),
                    'inputLength': len(dream_text),
                    'storyLength': len(final_story) if final_story else 0,
                    'videoGenerated': video_path is not None,  # æ–°å¢ï¼šè¦–é »ç”Ÿæˆç‹€æ…‹
                    'videoType': video_type if video_path else None,  # æ–°å¢ï¼šè¦–é »é¡å‹
                    'requestId': request_id[:20]
                }
            }
            
            print(f"âœ… å¤¢å¢ƒåˆ†æå®Œæˆ [ID: {request_id[:20]}...]")
            return jsonify(response)
            
        except Exception as e:
            print(f"âŒ åˆ†æéŒ¯èª¤ [ID: {request_id[:20]}...]: {e}")
            return jsonify({'error': 'è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤'}), 500
        
        finally:
            # æ¸…ç†è™•ç†ç‹€æ…‹
            self.request_lock = False
            self.processing_requests.discard(request_id)
            print(f"ğŸ”“ é‡‹æ”¾è«‹æ±‚é– [ID: {request_id[:20]}...]")

    def share_result(self):
        data = request.json
        
        if not data or 'finalStory' not in data:
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦çš„å¤¢å¢ƒåˆ†ææ•¸æ“š'}), 400
        
        try:
            share_id = self._save_dream_result(data)
            
            if not share_id:
                return jsonify({'error': 'å‰µå»ºåˆ†äº«å¤±æ•—'}), 500
            
            share_url = url_for('view_shared', share_id=share_id, _external=True)
            
            return jsonify({
                'shareId': share_id, 
                'shareUrl': share_url,
                'timestamp': int(time.time())
            })
            
        except Exception as e:
            print(f"âŒ åˆ†äº«è™•ç†éŒ¯èª¤: {e}")
            return jsonify({'error': 'è™•ç†åˆ†äº«è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤'}), 500

    def view_shared(self, share_id):
        try:
            share_file = os.path.join(self.static_dir, 'shares', f"{share_id}.json")
            
            if not os.path.exists(share_file):
                return jsonify({'error': 'æ‰¾ä¸åˆ°è©²åˆ†äº«å…§å®¹'}), 404
            
            with open(share_file, 'r', encoding='utf-8') as f:
                share_data = json.load(f)
            
            return render_template('shared.html', data=share_data)
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥åˆ†äº«å…§å®¹éŒ¯èª¤: {e}")
            return jsonify({'error': 'è¼‰å…¥åˆ†äº«å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤'}), 500

    def run(self, debug=False, host='0.0.0.0', port=5002):
        """å•Ÿå‹•æ‡‰ç”¨"""
        print("ğŸš€ å•Ÿå‹•å¤¢å¢ƒåˆ†ææ‡‰ç”¨...")
        self.app.run(debug=debug, host=host, port=port, threaded=True)


# ä¸»ç¨‹å¼å…¥å£
if __name__ == '__main__':
    dream_analyzer = DreamAnalyzer()
    dream_analyzer.run(debug=False)
